{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4 - rekomendacje dla portali informacyjnych\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
    "   * więcej możesz poczytać tutaj: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab4`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install nltk sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mateu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importujemy wszystkie potrzebne pakiety\n",
    "\n",
    "import codecs\n",
    "from collections import defaultdict # mozesz uzyc zamiast zwyklego slownika, rozwaz wplyw na czas obliczen\n",
    "import math\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# mozesz uzyc do obliczania najbardziej podobnych tekstow zamiast liczenia \"na piechote\"\n",
    "# ale pamietaj o dostosowaniu formatu danych\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiujemy potrzebne zmienne\n",
    "\n",
    "PATH = './MINDsmall_train'\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    }
   ],
   "source": [
    "# wczytujemy metadane artykułów\n",
    "\n",
    "def parse_news_entry(entry):\n",
    "    news_id, category, subcategory, title, abstract = entry.split('\\t')[:5]\n",
    "    return {\n",
    "        'news_id': news_id,\n",
    "        'category': category,\n",
    "        'subcategory': subcategory,\n",
    "        'title': title,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "\n",
    "def get_news_metadata():\n",
    "    with codecs.open(f'{PATH}/news.tsv', 'r', 'UTF-8') as f:\n",
    "        raw = [x for x in f.read().split('\\n') if x]\n",
    "        parsed_entries = [parse_news_entry(entry) for entry in raw]\n",
    "        return {x['news_id']: x for x in parsed_entries}\n",
    "\n",
    "news = get_news_metadata()\n",
    "news_ids = sorted(list(news.keys()))\n",
    "news_indices = {x[1]: x[0] for x in enumerate(news_ids)}\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2. - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizujemy teksty na potrzeby dalszego przetwarzania\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in STOPWORDS]\n",
    "    return text\n",
    "\n",
    "def stem_texts(corpus):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return [[stemmer.stem(word) for word in preprocess_text(text)] for text in corpus]\n",
    "\n",
    "texts = [news[news_id]['abstract'] for news_id in news_ids]\n",
    "stemmed_texts = stem_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "think realli good team team realli special good thing group close brian schmetzer\n"
     ]
    }
   ],
   "source": [
    "# porownajmy teksty przed i po przetworzeniu\n",
    "print(texts[2] + '\\n')\n",
    "print(' '.join(stemmed_texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41691\n"
     ]
    }
   ],
   "source": [
    "# tworzymy liste wszystkich slow w korpusie\n",
    "\n",
    "def get_all_words_sorted(corpus):\n",
    "    # generujemy posortowana alfabetycznie liste wszystkich slow (tokenow)\n",
    "    return sorted(list(set([word for text in corpus for word in text])))\n",
    "\n",
    "wordlist = get_all_words_sorted(stemmed_texts)\n",
    "word_indices = {x[1]: x[0] for x in enumerate(wordlist)}\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe tekstow, w ktorych wystapilo kazde ze slow\n",
    "# pamietaj, ze jesli slowo wystapilo w danym tekscie wielokrotnie, to liczymy je tylko raz\n",
    "\n",
    "def get_document_frequencies(corpus):\n",
    "    result = defaultdict(int)\n",
    "    for text in corpus:\n",
    "        for word in set(text):\n",
    "            result[word] += 1\n",
    "    return result\n",
    "\n",
    "document_frequency = get_document_frequencies(stemmed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1639"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe wystapien kazdego slowa w kazdym tekscie\n",
    "\n",
    "def get_term_frequencies(corpus, news_indices):\n",
    "    term_frequency = defaultdict(lambda: defaultdict(int))\n",
    "    for news_id, text in zip(news_indices, corpus):\n",
    "        for word in text:\n",
    "            term_frequency[news_id][word] += 1\n",
    "    return term_frequency\n",
    "\n",
    "term_frequency = get_term_frequencies(stemmed_texts, news_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'think': 1,\n",
       "             'realli': 2,\n",
       "             'good': 2,\n",
       "             'team': 2,\n",
       "             'special': 1,\n",
       "             'thing': 1,\n",
       "             'group': 1,\n",
       "             'close': 1,\n",
       "             'brian': 1,\n",
       "             'schmetzer': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "term_frequency[news_ids[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy metryke tf_idf\n",
    "\n",
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    tf_idf = defaultdict(lambda: defaultdict(float))\n",
    "    for news_id, text in term_frequency.items():\n",
    "        for word, frequency in text.items():\n",
    "            tf_idf[news_id][word] = frequency * math.log(corpus_size / document_frequency[word])\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'think': 4.360459856758821,\n",
       "             'realli': 9.707261090572182,\n",
       "             'good': 8.085400658139525,\n",
       "             'team': 6.157356388748834,\n",
       "             'special': 4.765161897298483,\n",
       "             'thing': 4.110503432421125,\n",
       "             'group': 4.261685870235309,\n",
       "             'close': 3.9244235881453897,\n",
       "             'brian': 5.624739267315748,\n",
       "             'schmetzer': 9.458800731274183})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "tf_idf[news_ids[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 3. - Podobieństwo tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.554262136437274"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obliczmy odleglosc miedzy dwoma artykulami\n",
    "# przetestuj rozne metryki odleglosci i wybierz najlepsza\n",
    "\n",
    "def calculate_distance(tf_idf, id1, id2, metric='euclidean'):\n",
    "    if metric == 'euclidean':\n",
    "        return math.sqrt(sum([(tf_idf[id1][word] - tf_idf[id2][word]) ** 2 for word in tf_idf[id1] if word in tf_idf[id2]]))\n",
    "    elif metric == 'cosine':\n",
    "        return sum([tf_idf[id1][word] * tf_idf[id2][word] for word in tf_idf[id1] if word in tf_idf[id2]])\n",
    "    elif metric == 'taxicab':\n",
    "        return sum([abs(tf_idf[id1][word] - tf_idf[id2][word]) for word in tf_idf[id1] if word in tf_idf[id2]])\n",
    "    elif metric == 'minkowski':\n",
    "        return sum([abs(tf_idf[id1][word] - tf_idf[id2][word]) ** 3 for word in tf_idf[id1] if word in tf_idf[id2]]) ** (1 / 3)\n",
    "    elif metric == 'chebyshev':\n",
    "        return max([abs(tf_idf[id1][word] - tf_idf[id2][word]) for word in tf_idf[id1] if word in tf_idf[id2]])\n",
    "    elif metric == 'jaccard':\n",
    "        return sum([min(tf_idf[id1][word], tf_idf[id2][word]) for word in tf_idf[id1] if word in tf_idf[id2]]) / sum([max(tf_idf[id1][word], tf_idf[id2][word]) for word in tf_idf[id1] if word in tf_idf[id2]])\n",
    "    elif metric == 'dice':\n",
    "        return 2 * sum([min(tf_idf[id1][word], tf_idf[id2][word]) for word in tf_idf[id1] if word in tf_idf[id2]]) / (sum([tf_idf[id1][word] for word in tf_idf[id1] if word in tf_idf[id2]]) + sum([tf_idf[id2][word] for word in tf_idf[id1] if word in tf_idf[id2]]))\n",
    "    raise ValueError('Unknown metric')\n",
    "\n",
    "calculate_distance(tf_idf, news_ids[2], news_ids[1], 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean: 19.554262136437274\n",
      "cosine: 18.956518849033042\n",
      "taxicab: 57.37711458653518\n",
      "minkowski: 14.252398792095919\n",
      "chebyshev: 9.707261090572182\n",
      "jaccard: 0.050924453270036106\n",
      "dice: 0.09691363277651513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a table with results for all metrics\n",
    "metrics = 'euclidean', 'cosine', 'taxicab', 'minkowski', 'chebyshev', 'jaccard', 'dice'\n",
    "results = [calculate_distance(tf_idf, news_ids[2], news_ids[1], metric) for metric in metrics]\n",
    "\n",
    "# Create a string printing the table\n",
    "results_string = ''\n",
    "for metric, result in zip(metrics, results):\n",
    "  results_string += f'{metric}: {result}\\n'\n",
    "\n",
    "# Print the table\n",
    "print(results_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: N58544, text: A MAN claims he has created a car that might solve the world's traffic congestion problems. Rick Woodbury from Spokane, Washington USA, is the president, founder and sole employee of 'Commuter Cars.' The carmaker's flagship model is the 2005 super slim two-seater Tango T600, a high-performance electric car that preceded Tesla. Rick told BTV: \"I started this company 21 years ago   it was based on an idea that I came up with in 1982.\" He was inspired by the shocking traffic congestion he had to face on a daily basis. \"I used to drive a Porsche from Beverly Hills to Hermosa Beach every day and the traffic was horrendous,\" explained Rick. What really made Rick think about a solution was the fact that in most of the cars he would see in his commute were occupied by lone drivers. \"I noticed that everybody around me was a single occupant in a car, taking up the whole lane,\" Rick said. Living and working in Los Angeles also helped inspire Rick's unique creation. \"I thought, everyone wants to get from point A to point B efficiently, and in cities like Los Angeles there's really no centre, there's no hub, everybody goes everywhere,\" explained Rick. For him, there is a simple solution, and that is reducing the width that vehicles take up on the road so that 2 can fit comfortably in a single lane. \"I don't think there's any other answer except doubling lane capacity,\" said Rick. With a length of 102 inches this micro car can be parked just about anywhere, just like you would with a motorbike. Rick said: \"The length is the same width of a semi-truck so I can park perpendicular to the curb.\"The selling point of this vehicle is that it can drive in-between cars better than any other car. \"The coolest feature for me to that it can get through traffic faster than any car in history,\" said Rick. Much like a motorbike, the Tango T500 can drive right up to the stop line of traffic lights.\n",
      "\n",
      "5 most similar:\n",
      "\n",
      "id: N64287, text: SEBASTIAN COUNTY (KFSM)   Sebastian County Justice of the Peace Rick Reedy passed away suddenly from a heart attack on a hunting trip this past weekend. Karla and Rick Reedy have known each other since they were just teenagers. \"It would have been 45 years. I met him when I was 16 and we got married when I was 18 and so we've been married 42 years,\" Karla said. Not only was Rick a father, ...\n",
      "\n",
      "id: N33947, text: This is the terrifying moment a wild elephant crushed a passing car after refusing to let it drive by on a mountain road. The jumbo noticed the grey Toyota approaching on a bend in Nakhon Rathashima, northeastern Thailand, last Tuesday (October 29) afternoon. The large bull stomped over to the vehicle and kneeled on the bonnet before walking behind and laying its huge body across the rear windscreen. Shocked driver Phassakorn Niltarach was left cowering behind the steering wheel while the beast smashed the glass and dented the bodywork and damaged the mirrors. The vehicle eventually managed to move forward after the ordeal, lasting more than a minute. The woman who recorded the video is heard saying \"are you crazy?\" She then adds: ''You should have driven away earlier before the elephant had chance to do that.\" Speaking after, the woman, who wanted to remain anonymous, said: ''I was so shocked when the elephant walked to the car and I was even more surprised when the elephant tried to sit on that car. \"It's lucky that the driver managed to get away safely. I want this to be an example because people should stay well away from the elephants. They might appear friendly but they could hurt you.\" Wildlife rangers in the Khao Yai National Park said that the elephant was a 35-year-old male named Deu. They warned drivers to take extra care when driving through the mountain roads, where elephants often roam. Park ranger Kanchit Sarinpawan said that elephants often venture out of the forest around October when the country's monsoon rainy season ends and the dry and cool period begins. The official reassured motorists that the elephant's behaviour was ''just his way of greeting tourists.\"\n",
      "\n",
      "id: N10664, text: An officer is in stable condition after a crash involving at least four cars in Calvert County, Maryland Thursday morning, officials said. Around 6 a.m., the on-duty deputy was sitting at a traffic light in his unmarked Sheriff Deputy's car on Route 4 when he was hit from behind, causing a chain reaction. After the deputy was hit, his car struck the car in front of him and then that car crashed into the car in front of it. The deputy has not...\n",
      "\n",
      "id: N60776, text: WASHINGTON - President Donald Trump said Friday he would nominate Deputy Energy Secretary Dan Brouillette to helm the Department of Energy, following Rick Perry's announcement he would resign by the end of the year. \"Rick was a great Governor of Texas and a great Secretary of Energy....He is also my friend!\" Trump tweeted \"At the same time, I am pleased to nominate Deputy Secretary Dan Brouillette to be the new Secretary of Energy. Dan's...\n",
      "\n",
      "id: N1720, text: Traffic camera video of a violent rear-end injury crash involving three cars is a \"classic case of distracted driving,\" Overland Park Police Chief Frank Donchez said on Twitter Wednesday morning. The video shows a dark-colored car slam into the rear of white car. The force of the crash lifts the rear of the white car into the air, pushing it into another car in front of it. \"C'mon ...\n"
     ]
    }
   ],
   "source": [
    "# wyznaczmy k najpodobniejszych tekstow do danego\n",
    "# pamietaj o odpowiedniej kolejnosci sortowania w zaleznosci od wykorzystanej metryki\n",
    "# pamietaj, zeby wsrod podobnych tekstow nie bylo danego\n",
    "\n",
    "def get_k_most_similar_news(tf_idf, n_id, k, metric='euclidean'):\n",
    "    if metric == 'euclidean':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric))[:k]\n",
    "    elif metric == 'cosine':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric), reverse=True)[:k]\n",
    "    elif metric == 'taxicab':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric))[:k]\n",
    "    elif metric == 'minkowski':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric))[:k]\n",
    "    elif metric == 'chebyshev':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric))[:k]\n",
    "    elif metric == 'jaccard':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric), reverse=True)[:k]\n",
    "    elif metric == 'dice':\n",
    "        return sorted([id for id in tf_idf if id != n_id], key=lambda x: calculate_distance(tf_idf, n_id, x, metric), reverse=True)[:k]    \n",
    "\n",
    "def print_k_most_similar_news(tf_idf, n_id, k, corpus, news_indices, metric='euclidean'):\n",
    "    similar = get_k_most_similar_news(tf_idf, n_id, k, metric)\n",
    "    print(f'id: {n_id}, text: {corpus[news_indices[n_id]]}')\n",
    "    print(f'\\n{k} most similar:')\n",
    "    for s_id in similar:\n",
    "        print(f'\\nid: {s_id}, text: {corpus[news_indices[s_id]]}')\n",
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[42337], 5, texts, news_indices, 'cosine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
