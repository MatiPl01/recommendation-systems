{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 3 - segmentacje\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://data.world/socialmediadata/beeradvocate\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab3`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install gensim==3.8.3 scikit-learn==1.3.2 wordcloud==1.8.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Zadziałało na wersji Pythona 3.9.12 (na 3.11.x nie działa)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.8.3\n",
      "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
      "Collecting scikit-learn==1.3.2\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-macosx_12_0_arm64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wordcloud==1.8.1\n",
      "  Downloading wordcloud-1.8.1.tar.gz (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.3.2) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/mateu/.local/lib/python3.9/site-packages (from scikit-learn==1.3.2) (1.2.0)\n",
      "Requirement already satisfied: pillow in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from wordcloud==1.8.1) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from wordcloud==1.8.1) (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud==1.8.1) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud==1.8.1) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud==1.8.1) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud==1.8.1) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud==1.8.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/mateu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud==1.8.1) (3.0.4)\n",
      "Building wheels for collected packages: gensim, wordcloud\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.3-cp39-cp39-macosx_11_0_arm64.whl size=24148280 sha256=c6e4855e0c8268057a539e432d17eb42bca93d3fb078f8a6f66477070a40bb41\n",
      "  Stored in directory: /Users/mateu/Library/Caches/pip/wheels/ca/5d/af/618594ec2f28608c1d6ee7d2b7e95a3e9b06551e3b80a491d6\n",
      "  Building wheel for wordcloud (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wordcloud: filename=wordcloud-1.8.1-cp39-cp39-macosx_11_0_arm64.whl size=151628 sha256=969b04568fd33aa3b6d972354955898655626c1631afec0adeba1b010f18c6fc\n",
      "  Stored in directory: /Users/mateu/Library/Caches/pip/wheels/f9/7a/dd/06ef8b5dfe5483f6204133c08eeb16c287cc2c05e290ae2fc0\n",
      "Successfully built gensim wordcloud\n",
      "Installing collected packages: wordcloud, scikit-learn, gensim\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "Successfully installed gensim-3.8.3 scikit-learn-1.3.2 wordcloud-1.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim==3.8.3 scikit-learn==1.3.2 wordcloud==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importujemy potrzebne pakiety\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'socialmediadata-beeradvocate/data/beer_reviews.csv'\n",
    "SCORE_THRESHOLD = 4 # recenzje z co najmniej taka ocena wezmiemy pod uwage\n",
    "VECTOR_SIZE = 20 # jak dlugie powinny byc wektory osadzen uzytkownikow\n",
    "SEGMENTS_COUNT = 10 # na ile segmentow chcemy podzielic populacje uzytkownikow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytujemy dane\n",
    "\n",
    "def parse_headers(column_names):\n",
    "    beer_id_column =  column_names.index('beer_beerid')\n",
    "    beer_style_column = column_names.index('beer_style')\n",
    "    user_id_column =  column_names.index('review_profilename')\n",
    "    score_column =  column_names.index('review_overall')\n",
    "    return beer_id_column, beer_style_column, user_id_column, score_column\n",
    "\n",
    "def parse_review(line, beer_id_column, beer_style_column, user_id_column, score_column):\n",
    "    return line[beer_id_column], line[beer_style_column], line[user_id_column], float(line[score_column])\n",
    "\n",
    "def read_and_parse_reviews(path, score_threshold):\n",
    "    with codecs.open(PATH, 'r', 'UTF-8') as datafile:\n",
    "        data_reader = csv.reader(datafile)\n",
    "        beer_id_column, beer_style_column, user_id_column, score_column = parse_headers(next(data_reader))\n",
    "    \n",
    "        users_favourite_beers = defaultdict(list)\n",
    "        for review in data_reader:\n",
    "            beer_id, _, user_id, score = parse_review(review, beer_id_column, beer_style_column, user_id_column, score_column)\n",
    "            if score >= score_threshold:\n",
    "                users_favourite_beers[user_id].append(beer_id)\n",
    "\n",
    "    return users_favourite_beers\n",
    "\n",
    "def get_beer_id_to_style_mapping(path):\n",
    "    with codecs.open(PATH, 'r', 'UTF-8') as datafile:\n",
    "        data_reader = csv.reader(datafile)\n",
    "        beer_id_column, beer_style_column, user_id_column, score_column = parse_headers(next(data_reader))\n",
    "        beer_styles = {}\n",
    "        for review in data_reader:\n",
    "            beer_id, beer_style, _, _ = parse_review(review, beer_id_column, beer_style_column, user_id_column, score_column)\n",
    "            beer_styles[beer_id] = beer_style\n",
    "        return beer_styles\n",
    "\n",
    "# otrzymujemy slownik - mapowanie z user_id na liste ulubionych beer_ids\n",
    "users_favourite_beers = read_and_parse_reviews(PATH, SCORE_THRESHOLD)\n",
    "\n",
    "# dodatkowo przygotujmy sobie slownik mapujacy id piwa na nazwe stylu\n",
    "beer_styles = get_beer_id_to_style_mapping(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2. - osadzenia użytkowników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trenujemy model osadzajacy piwa i uzytkownikow w przestrzeni\n",
    "\n",
    "model = Word2Vec(sentences=users_favourite_beers.values(), size=VECTOR_SIZE, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na podstawie modelu obliczamy wektorowa reprezentacje uzytkownikow\n",
    "\n",
    "def get_mean_vector_for_user(user_favourite_beers, model, vector_size):\n",
    "    # inicjalizujemy wektor zerami\n",
    "    v = np.zeros(vector_size)\n",
    "    \n",
    "    # iterujemy po liscie user_favourite_beers, odczytujemy z modelu wektor reprezentujacy kazde piwo i dodajemy do wektora\n",
    "    #   uzyj: model.wv[beer_id]\n",
    "    for beer_id in user_favourite_beers:\n",
    "        v += model.wv[beer_id]\n",
    "    \n",
    "    # normalizujemy wektor - dzielimy kazda wspolrzedna przez liczbe piw\n",
    "    v /= len(user_favourite_beers)\n",
    "    \n",
    "    return v\n",
    "\n",
    "def get_mean_user_vectors(users_favourite_beers, model, vector_size):\n",
    "    # korzystajac z powyzszej funkcji, tworzymy slownik {user_id -> vector}\n",
    "    mean_users_vectors = {}\n",
    "    \n",
    "    for user_id, user_favourite_beers in users_favourite_beers.items():\n",
    "        mean_users_vectors[user_id] = get_mean_vector_for_user(user_favourite_beers, model, vector_size)\n",
    "    \n",
    "    return mean_users_vectors\n",
    "\n",
    "user_vectors = get_mean_user_vectors(users_favourite_beers, model, VECTOR_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 3. - klasteryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klastrujemy uzytkownikow - mozemy uzyc wielu algorytmow, np k-means, agglomerative, BIRCH, ...\n",
    "\n",
    "def get_users_segmentation(user_vectors, vector_size, segments_count, clustering_algorithm):\n",
    "    # rozkladamy slownik user_vectors na liste uzytkownikow i liste wektorow\n",
    "    #  - wazne jest zachowanie tej samej kolejnosci w obu listach\n",
    "    users = user_vectors.keys()\n",
    "    vectors = user_vectors.values()\n",
    "    \n",
    "    # zamieniamy liste wektorow w macierz\n",
    "    users_array = np.stack(tuple(vectors), axis=0)\n",
    "    # zaimplementuj wsparcie dla co najmniej jednego algorytmu wiecej\n",
    "    if clustering_algorithm == 'agglomerative':\n",
    "        clustering = AgglomerativeClustering(n_clusters=segments_count).fit_predict(users_array)\n",
    "        # clustering to lista przypisanych klastrow - i-ty element to klaster, do ktorego nalezy i-ty wektor\n",
    "    \n",
    "    segmentation = {}\n",
    "    # jesli nie pomieszalismy kolejnosci w listach, to mozemy odzyskac mapping user_id -> cluster\n",
    "    \n",
    "    for i, user_id in enumerate(users):\n",
    "        segmentation[user_id] = clustering[i]\n",
    "    \n",
    "    return segmentation\n",
    "\n",
    "segmentation = get_users_segmentation(user_vectors, VECTOR_SIZE, SEGMENTS_COUNT, 'agglomerative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 15540,\n",
       "         8: 3086,\n",
       "         7: 3112,\n",
       "         2: 1307,\n",
       "         9: 2321,\n",
       "         3: 1113,\n",
       "         6: 1074,\n",
       "         5: 818,\n",
       "         1: 964,\n",
       "         4: 951})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obliczamy licznosci segmentow\n",
    "\n",
    "def get_segment_sizes(segmentation):\n",
    "    return Counter(segmentation.values())\n",
    "\n",
    "get_segment_sizes(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'silhouette_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mateu/Education/Studies/Term7/recommendation-systems/lab3/lab3_segmentation.ipynb Cell 14\u001b[0m line \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mateu/Education/Studies/Term7/recommendation-systems/lab3/lab3_segmentation.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m users, user_vectors_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39muser_vectors\u001b[39m.\u001b[39mitems())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mateu/Education/Studies/Term7/recommendation-systems/lab3/lab3_segmentation.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m segments_list \u001b[39m=\u001b[39m [segmentation[u] \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m users]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mateu/Education/Studies/Term7/recommendation-systems/lab3/lab3_segmentation.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mean_silhouette \u001b[39m=\u001b[39m silhouette_score(user_vectors_list, segments_list, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mateu/Education/Studies/Term7/recommendation-systems/lab3/lab3_segmentation.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m per_sample_silhouettes \u001b[39m=\u001b[39m silhouette_samples(user_vectors_list, segments_list, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'silhouette_score' is not defined"
     ]
    }
   ],
   "source": [
    "# obliczamy metryke jakosci segmentacji\n",
    "# metryka przyjmuje wartosci [-1, 1] - im wiecej, tym lepiej\n",
    "users, user_vectors_list = zip(*user_vectors.items())\n",
    "segments_list = [segmentation[u] for u in users]\n",
    "\n",
    "mean_silhouette = silhouette_score(user_vectors_list, segments_list, metric='euclidean')\n",
    "per_sample_silhouettes = silhouette_samples(user_vectors_list, segments_list, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rysujemy wykres\n",
    "def plot_histogram(values, mean_value):\n",
    "    plt.hist(values, color='c', edgecolor='k', alpha=0.65)\n",
    "    plt.axvline(mean_value, linestyle='dashed', linewidth=1)\n",
    "    _, plot_height = plt.ylim()\n",
    "    plt.text(0, plot_height*1.05, 'Mean: {:.3f}'.format(mean_value))\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(per_sample_silhouettes, mean_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 4. - opisy segmentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy popularnosc styli w segmentach oraz w calej populacji\n",
    "#  - czyli jak czesto kazdy ze styli wystepowal\n",
    "\n",
    "def get_per_segment_styles_popularity(users_favourite_beers, beer_styles, segmentation, segments_count):\n",
    "    beer_styles_per_segment = {i: defaultdict(lambda: 0) for i in range(segments_count)}\n",
    "    # do tego slownika bedziemy sie odwolywac np. tak: beer_styles_per_segment[segment][style]\n",
    "    \n",
    "    for user, beers in users_favourite_beers.items():\n",
    "        # ...\n",
    "    \n",
    "    return beer_styles_per_segment\n",
    "\n",
    "def get_total_styles_popularity(beer_styles_per_segment):\n",
    "    total_popularity = defaultdict(lambda: 0)\n",
    "    # ...\n",
    "    return total_popularity\n",
    "\n",
    "per_segment_styles_popularity = get_per_segment_styles_popularity(users_favourite_beers, beer_styles, segmentation, SEGMENTS_COUNT)\n",
    "total_styles_popularity = get_total_styles_popularity(per_segment_styles_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotujmy dwa opisy segmentow - top N najpopularniejszych stylow w kazdym segmencie\n",
    "#   oraz top N unikalnych - to samo, ale popularnosc w segmencie dzielimy przez laczna popularnosc\n",
    "#   dla potrzeb wizualizacji w nastepnym kroku, zwroc slownik {segment_id -> {style_id -> frequency}}\n",
    "\n",
    "def most_popular_styles_per_segment(per_segment_styles_popularity, N):\n",
    "    # ...\n",
    "\n",
    "def most_distinctive_styles_per_segment(total_styles_popularity, per_segment_styles_popularity, N):\n",
    "    # ...\n",
    "     \n",
    "top_N = 50\n",
    "most_popular = most_popular_styles_per_segment(per_segment_styles_popularity, top_N)\n",
    "most_distinctive = most_distinctive_styles_per_segment(total_styles_popularity, per_segment_styles_popularity, top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 5. - wizualizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teraz mozemy zwizualizowac nasze segmentacje za pomoca word clouds\n",
    "#   zauwaz, jak roznia sie obie metody opisow segmentow oraz opisy miedzy poszczegolnymi segmentami\n",
    "\n",
    "def visualise_styles(most_popular, most_distinctive, segment_id):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    wordcloud1 = WordCloud(random_state=42, background_color='black', colormap='Set2')\n",
    "    wordcloud1.generate_from_frequencies(frequencies=most_popular[segment_id])\n",
    "    ax1.imshow(wordcloud1, interpolation=\"bilinear\")\n",
    "    wordcloud2 = WordCloud(random_state=42, background_color='black', colormap='Set2')\n",
    "    wordcloud2.generate_from_frequencies(frequencies=most_distinctive[segment_id])\n",
    "    ax2.imshow(wordcloud2, interpolation=\"bilinear\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "visualise_styles(most_popular, most_distinctive, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
